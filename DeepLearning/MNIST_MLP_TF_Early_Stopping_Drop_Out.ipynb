{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_MLP_TF_Early_Stopping_Drop_Out.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-P2TKVJ05bT"
      },
      "source": [
        "# **Tensorflow를 이용한 MNIST Multi Layer Perceptron with Early Stopping & Drop out 실습하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1XJefscc1JIZ",
        "outputId": "8e162023-82e3-4285-b5c4-7cf04119a0a4"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw8BV_gNdftv"
      },
      "source": [
        "## **MLP Architecture**\r\n",
        "input layer, hidden layer h1, h2, output layer logit. 10개의 output 노드가 10개의 softmax를 통해 prediction으로 변하고, 이 prediction을 y label과 비교를 함으로서 cross entropy를 가짐. 이 cross entropy를 최저로 가지는 것을 목적 함수로 해 adam optimizer를 돌려 architecture를 최적화 한다.\r\n",
        "<p align=\"center\"><img src=\"https://camo.githubusercontent.com/993b7481ad0a8441d1e3393fc6fc493358be08fc/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6d696e73756b2d68656f2f646565706c6561726e696e672f6d61737465722f696d672f64726f706f75742e706e67\" height=\"450\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSb16qBXehGr"
      },
      "source": [
        "## **MNIST Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a8XxbIz1OxT"
      },
      "source": [
        "# mnist data load\r\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXseMkLqgAgo",
        "outputId": "26adc4ce-0b20-4186-99f0-c05a8debf4ac"
      },
      "source": [
        "print(x_train.shape)\r\n",
        "print(y_train.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Exk3QmRgtWg"
      },
      "source": [
        "train data는 **60000**개의 sample을 가진다.  \r\n",
        "test data는 **60000**개의 sample을 가진다.  \r\n",
        "모든 data는 **28*28** pixels\r\n",
        "  \r\n",
        "  MNIST는 사람이 손으로 쓴 숫자로 이루어지는 대형 데이터베이스다. 아래 이미지는 28*28 pexels로 구성되며 흰색과 검정색으로 이루어진 gray scale image [0 to 255]다.\r\n",
        "\r\n",
        "<p align=\"center\"><img src=\"https://mblogthumb-phinf.pstatic.net/MjAxOTAxMTZfMTg4/MDAxNTQ3NjQ2MzI5OTQ0.257wNc_ErINd8lMDTaor5ls3UP0T2lD1yV_KoC_Ik6Mg.OGV-dRDH3m9U48AhlEaptGI9D8tHsvP-OZbPYyofin4g.PNG.garden2040/Figure_1.png?type=w800\" height=\"300\"></p>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlB6x31Ji1nW"
      },
      "source": [
        "## **Split Train data into Train and Validation data**\r\n",
        "training 중 validation은 다음과 같은 장점을 가진다.  \r\n",
        "1. training 중 validation score를 이용해 hyper parameter를 수정할 수 있다.\r\n",
        "2. training score가 올라가는 동안 validation score(accuracy)가 변화가 없을 시 **early stopping**을 제공한다. (**overfitting** 방지)\r\n",
        "\r\n",
        "\r\n",
        "train data 60000개 중 10000개의 데이터를 validation data로 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVOmIoDAgMUg"
      },
      "source": [
        "x_val = x_train[50000:60000]\r\n",
        "x_train = x_train[0:50000]\r\n",
        "y_val = y_train[50000:60000]\r\n",
        "y_train = y_train[0:50000]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rb_nIAvlUZX"
      },
      "source": [
        "## **Reshape**\r\n",
        "\r\n",
        "모든 pixels을 hidden layer에 fully connected (완전 연결)하려면 (28,28)을 (28x28, 1)로 reshape 해야한다.  \r\n",
        "== row * col shape를 28x28개의 item이 있는 행렬로 flatten 해야 한다.\r\n",
        "\r\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\" height=\"420\"></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CahXAhGkslK",
        "outputId": "414de637-18ca-44df-c796-edd7a2ceeb82"
      },
      "source": [
        "x_train = x_train.reshape(50000, 784)\r\n",
        "x_val = x_val.reshape(10000, 784)\r\n",
        "x_test = x_test.reshape(10000, 784)\r\n",
        "\r\n",
        "print(x_train.shape)\r\n",
        "print(x_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkN0lQ7fGr8c",
        "outputId": "4ea4648b-1f1e-420f-d3d9-2e4619fede36"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
              "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
              "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
              "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
              "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
              "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
              "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
              "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
              "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
              "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAj41fJhHWqs"
      },
      "source": [
        "## **Normalize data**\r\n",
        "정규화는 모든 입력 feature를 동일한 범위로 만들어 분산을 줄임으로써 학습 속도를 향상시키거나, 더 나은 성능을 돕는다.  \r\n",
        "MNIST 데이터 셋은 [0 to 255]의 값을 가지므로 정규화를 할 시 variance만 줄일 수 있다.  \r\n",
        "  \r\n",
        "MNIST는 MLP architecture로 표준화하는 것보다 정규화하는 것이 더 낫다. ReLU가 peed forward와 back propagation에서 모두 0을 다르게 다루기 때문. MNIST에서 [1 to 255]는 손으로 쓴 것을 의미하므로 아무 것도 쓰지 않은 0을 다르게 취급하는 것이 중요하다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZu6Ya6vG52p"
      },
      "source": [
        "x_train = x_train.astype('float32')\r\n",
        "x_val = x_val.astype('float32')\r\n",
        "x_test = x_test.astype('float32')\r\n",
        "\r\n",
        "# 255로 나누게 되면 [0 to 1]의 값을 가지게 됨\r\n",
        "gray_scale = 255 \r\n",
        "x_train /= gray_scale\r\n",
        "x_val /= gray_scale\r\n",
        "x_test /= gray_scale"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlLyNsfwKOdE"
      },
      "source": [
        "## **Label to One Hot Encoding Value**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxwwI13xJNFc"
      },
      "source": [
        "# class를 10으로 해 y값들을 모두 One Hot Encoding한다. (1~10까지의 숫자를 0~9번 노드로)\r\n",
        "num_classes = 10\r\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\r\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90O9Z3yMKppV",
        "outputId": "d37084d7-ed2d-4123-d963-8ef3890c18db"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0t7hHhJK9ir"
      },
      "source": [
        "## **Tensorflow MLP Graph**\r\n",
        "\r\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\" height=\"450\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jTY4xvxTk6h"
      },
      "source": [
        "## **Drop Out**\r\n",
        "Drop out은 hidden layer에서 일부 node를 비활성화하는 것으로(model의 variance를 낮추는 효과가 있다), overfitting을 예방한다.  model을 train 할 때만 사용한다.\r\n",
        "Drop out은 모든 mini batch에서 서로 다른 활성화된 node가 있기 때문에 ensemble 효과가 있다.  \r\n",
        "Drop out mini batch의 결과로 딥러닝 모델에서 변수를 조정하는 것은 random forest 접근법과 유사하다.  \r\n",
        "keep_prob는 drop out 비율에 대한 tensorflow placehoder다.\r\n",
        "\r\n",
        "<p align=\"center\"><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/08/eba93f5a75070f0fbb9d86bec8a009e9.png\" height=\"350\"></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z0tcz4-K4N-"
      },
      "source": [
        "# tensorflow 2.x에서는 session을 정의하고 run을 수행하는 과정이 생략되고 바로 실행되는 형태가 되었으므로 아래와 같은 옵션을 추가해준다.\r\n",
        "tf.compat.v1.disable_eager_execution()\r\n",
        "\r\n",
        "# input으로 784개\r\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, 784])\r\n",
        "\r\n",
        "# [None, 10] : 총 몇 개의 sample이 들어올지 모르겠지만 10개의 숫자를 구분할 것이다.\r\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, 10])\r\n",
        "\r\n",
        "keep_prob = tf.compat.v1.placeholder(tf.float32)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ClYdZuV1fd"
      },
      "source": [
        "hidden layer 2에서 drop out을 사용한다.  \r\n",
        "keep_prob는 train이나 test에서 채워진다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkxqxvKlL7r-"
      },
      "source": [
        "def mlp(x):\r\n",
        "  # hidden layer1\r\n",
        "  w1 = tf.Variable(tf.compat.v1.random_uniform([784, 256])) # 784개의 input을 받고, 256개의 node를 만든다.\r\n",
        "  b1 = tf.Variable(tf.zeros([256]))\r\n",
        "  h1 = tf.nn.relu(tf.matmul(x, w1) + b1) # activation func : ReLU\r\n",
        "  # hidden layer2\r\n",
        "  w2 = tf.Variable(tf.compat.v1.random_uniform([256, 128])) # 256개의 input을 받고, 128개의 node를 만든다.\r\n",
        "  b2 = tf.Variable(tf.zeros([128]))\r\n",
        "  h2 = tf.nn.relu(tf.matmul(h1, w2) + b2) # activation func : ReLU\r\n",
        "  h2_drop = tf.nn.dropout(h2, keep_prob) # drop out\r\n",
        "  # output layer\r\n",
        "  w3 = tf.Variable(tf.compat.v1.random_uniform([128, 10])) # 128개의 input을 받고, 10개의 node를 만든다.\r\n",
        "  b3 = tf.Variable(tf.zeros([10]))\r\n",
        "  logits = tf.matmul(h2, w3) + b3\r\n",
        "\r\n",
        "  return logits"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V0El7pZPICS"
      },
      "source": [
        "cross entropy를 하기 위해 softmax를 많이 사용한다. 이 logic value를 softmax에 넣어서 prediction을 받는다. 이 cross entropy를 minimize하는 것이 adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODT4NRlKN_GQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6811e70-0eb8-43c3-87e4-4accd4461f68"
      },
      "source": [
        "logits = mlp(x)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9btcW1WePJrV"
      },
      "source": [
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXvOUMDOSvbh"
      },
      "source": [
        "train_op = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDcqCcMjNdfn"
      },
      "source": [
        "## **Early Stopping**\r\n",
        "학습 회차별로, 학습의 정확도는 계속 올라가더라도 검증의 손실은 overfitting으로 오히려 낮아지게 되는 지점이 있는데 이를 과적합으로 간주하여 학습을 종료하는 것을 Early Stopping이라고 한다.\r\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/early_stop.png\" height=\"350\"></p>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMfEEp21NduA"
      },
      "source": [
        "# initialize\r\n",
        "init = tf.global_variables_initializer()\r\n",
        "\r\n",
        "# Add ops to save and restore all the variables.\r\n",
        "saver = tf.train.Saver()\r\n",
        "\r\n",
        "# train hyperparameters\r\n",
        "epoch_cnt = 300\r\n",
        "batch_size = 1000 # 1000개씩 넣으면서 architecture 최적화\r\n",
        "iteration = len(x_train) // batch_size # input이 50000개니까 총 50번의 iteration\r\n",
        "\r\n",
        "earlystop_threshold = 5 # validation set accuracy가 내려갔다가 올라가는 경우가 있는데 이를 5번까지 기다려주겠다는 의미\r\n",
        "earlystop_cnt = 0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdnoLDxAWWpA"
      },
      "source": [
        "데이터를 제공할 때마다 drop out 비율에 대해 반드시 keep_prob를 채워야 한다.  \r\n",
        "hidden layer 2에서 node 10%를 삭제하기 위해 0.9를 train에서 사용한다.  \r\n",
        "test중에 dropout을 사용하지 않을 것이므로 test에서 1.0을 사용했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfOVIH8OVWyw",
        "outputId": "c54342d0-52c3-47a5-aafb-e7bc87c73642"
      },
      "source": [
        "# Start training\r\n",
        "with tf.Session() as sess:\r\n",
        "    # Run the initializer\r\n",
        "    sess.run(init)\r\n",
        "    prev_train_acc = 0.0\r\n",
        "    max_val_acc = 0.0\r\n",
        "    \r\n",
        "    for epoch in range(epoch_cnt):\r\n",
        "        avg_loss = 0.\r\n",
        "        start = 0; end = batch_size\r\n",
        "        \r\n",
        "        for i in range(iteration):\r\n",
        "            _, loss = sess.run([train_op, loss_op], feed_dict={x: x_train[start: end], y: y_train[start: end], keep_prob: 0.9}) # training 할 때 90%만 사용한다(keep_prob)\r\n",
        "            start += batch_size; end += batch_size\r\n",
        "            # Compute train average loss\r\n",
        "            avg_loss += loss / iteration\r\n",
        "            \r\n",
        "        # Validate model\r\n",
        "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\r\n",
        "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\r\n",
        "        # Calculate accuracy\r\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n",
        "        # train accuracy\r\n",
        "        cur_train_acc = accuracy.eval({x: x_train, y: y_train,keep_prob: 1.0}) # model을 평가할 때는 전부 다 사용(keep_prob)\r\n",
        "        # validation accuarcy\r\n",
        "        cur_val_acc = accuracy.eval({x: x_val, y: y_val, keep_prob: 1.0})\r\n",
        "        # validation loss\r\n",
        "        cur_val_loss = loss_op.eval({x: x_val, y: y_val,keep_prob: 1.0})\r\n",
        "        \r\n",
        "        print(\"epoch: \"+str(epoch) + \", train acc: \" + str(cur_train_acc) + \", val acc: \" + str(cur_val_acc) )\r\n",
        "        \r\n",
        "        if cur_val_acc < max_val_acc:\r\n",
        "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\r\n",
        "                if earlystop_cnt == earlystop_threshold:\r\n",
        "                    print(\"early stopped on \"+str(epoch))\r\n",
        "                    break\r\n",
        "                else:\r\n",
        "                    print(\"overfitting warning: \"+str(earlystop_cnt))\r\n",
        "                    earlystop_cnt += 1\r\n",
        "            else:\r\n",
        "                earlystop_cnt = 0\r\n",
        "        else:\r\n",
        "            earlystop_cnt = 0\r\n",
        "            max_val_acc = cur_val_acc\r\n",
        "            # Save the variables to file.\r\n",
        "            save_path = saver.save(sess, \"model/model.ckpt\")\r\n",
        "        prev_train_acc = cur_train_acc"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, train acc: 0.16508, val acc: 0.17\n",
            "epoch: 1, train acc: 0.71804, val acc: 0.7426\n",
            "epoch: 2, train acc: 0.85864, val acc: 0.8729\n",
            "epoch: 3, train acc: 0.87476, val acc: 0.8848\n",
            "epoch: 4, train acc: 0.88682, val acc: 0.8943\n",
            "epoch: 5, train acc: 0.89334, val acc: 0.902\n",
            "epoch: 6, train acc: 0.89786, val acc: 0.9045\n",
            "epoch: 7, train acc: 0.90288, val acc: 0.9079\n",
            "epoch: 8, train acc: 0.90562, val acc: 0.9086\n",
            "epoch: 9, train acc: 0.90686, val acc: 0.9061\n",
            "overfitting warning: 0\n",
            "epoch: 10, train acc: 0.90616, val acc: 0.9078\n",
            "epoch: 11, train acc: 0.90502, val acc: 0.9068\n",
            "epoch: 12, train acc: 0.90418, val acc: 0.9048\n",
            "epoch: 13, train acc: 0.89454, val acc: 0.8939\n",
            "epoch: 14, train acc: 0.907, val acc: 0.9057\n",
            "overfitting warning: 0\n",
            "epoch: 15, train acc: 0.90422, val acc: 0.9002\n",
            "epoch: 16, train acc: 0.91104, val acc: 0.9092\n",
            "epoch: 17, train acc: 0.91262, val acc: 0.9087\n",
            "overfitting warning: 0\n",
            "epoch: 18, train acc: 0.92084, val acc: 0.916\n",
            "epoch: 19, train acc: 0.90486, val acc: 0.9027\n",
            "epoch: 20, train acc: 0.91188, val acc: 0.9075\n",
            "overfitting warning: 0\n",
            "epoch: 21, train acc: 0.91686, val acc: 0.9161\n",
            "epoch: 22, train acc: 0.92978, val acc: 0.9246\n",
            "epoch: 23, train acc: 0.92938, val acc: 0.9249\n",
            "epoch: 24, train acc: 0.92032, val acc: 0.916\n",
            "epoch: 25, train acc: 0.88842, val acc: 0.891\n",
            "epoch: 26, train acc: 0.91386, val acc: 0.911\n",
            "overfitting warning: 0\n",
            "epoch: 27, train acc: 0.92306, val acc: 0.922\n",
            "overfitting warning: 1\n",
            "epoch: 28, train acc: 0.9232, val acc: 0.9199\n",
            "overfitting warning: 2\n",
            "epoch: 29, train acc: 0.92778, val acc: 0.9232\n",
            "overfitting warning: 3\n",
            "epoch: 30, train acc: 0.92218, val acc: 0.9159\n",
            "epoch: 31, train acc: 0.91624, val acc: 0.9109\n",
            "epoch: 32, train acc: 0.92762, val acc: 0.9259\n",
            "epoch: 33, train acc: 0.93444, val acc: 0.9284\n",
            "epoch: 34, train acc: 0.94274, val acc: 0.9333\n",
            "epoch: 35, train acc: 0.93162, val acc: 0.9221\n",
            "epoch: 36, train acc: 0.94016, val acc: 0.9325\n",
            "overfitting warning: 0\n",
            "epoch: 37, train acc: 0.89876, val acc: 0.8962\n",
            "epoch: 38, train acc: 0.90516, val acc: 0.8971\n",
            "overfitting warning: 0\n",
            "epoch: 39, train acc: 0.94214, val acc: 0.9268\n",
            "overfitting warning: 1\n",
            "epoch: 40, train acc: 0.94448, val acc: 0.9294\n",
            "overfitting warning: 2\n",
            "epoch: 41, train acc: 0.9423, val acc: 0.9271\n",
            "epoch: 42, train acc: 0.93118, val acc: 0.9146\n",
            "epoch: 43, train acc: 0.94012, val acc: 0.9254\n",
            "overfitting warning: 0\n",
            "epoch: 44, train acc: 0.94396, val acc: 0.9277\n",
            "overfitting warning: 1\n",
            "epoch: 45, train acc: 0.93156, val acc: 0.9153\n",
            "epoch: 46, train acc: 0.92714, val acc: 0.9123\n",
            "epoch: 47, train acc: 0.92708, val acc: 0.9105\n",
            "epoch: 48, train acc: 0.94962, val acc: 0.931\n",
            "overfitting warning: 0\n",
            "epoch: 49, train acc: 0.96076, val acc: 0.9409\n",
            "epoch: 50, train acc: 0.94868, val acc: 0.9326\n",
            "epoch: 51, train acc: 0.94516, val acc: 0.9302\n",
            "epoch: 52, train acc: 0.96058, val acc: 0.9421\n",
            "epoch: 53, train acc: 0.96324, val acc: 0.9421\n",
            "epoch: 54, train acc: 0.96382, val acc: 0.9427\n",
            "epoch: 55, train acc: 0.9606, val acc: 0.9384\n",
            "epoch: 56, train acc: 0.95856, val acc: 0.9366\n",
            "epoch: 57, train acc: 0.96624, val acc: 0.9464\n",
            "epoch: 58, train acc: 0.9641, val acc: 0.9436\n",
            "epoch: 59, train acc: 0.95858, val acc: 0.9402\n",
            "epoch: 60, train acc: 0.9751, val acc: 0.9515\n",
            "epoch: 61, train acc: 0.97822, val acc: 0.954\n",
            "epoch: 62, train acc: 0.97626, val acc: 0.9529\n",
            "epoch: 63, train acc: 0.97842, val acc: 0.9548\n",
            "epoch: 64, train acc: 0.97586, val acc: 0.9545\n",
            "epoch: 65, train acc: 0.9678, val acc: 0.9494\n",
            "epoch: 66, train acc: 0.95866, val acc: 0.9397\n",
            "epoch: 67, train acc: 0.96544, val acc: 0.9463\n",
            "overfitting warning: 0\n",
            "epoch: 68, train acc: 0.9792, val acc: 0.9559\n",
            "epoch: 69, train acc: 0.97468, val acc: 0.9532\n",
            "epoch: 70, train acc: 0.98292, val acc: 0.9574\n",
            "epoch: 71, train acc: 0.98216, val acc: 0.9559\n",
            "epoch: 72, train acc: 0.98192, val acc: 0.9555\n",
            "epoch: 73, train acc: 0.9725, val acc: 0.9496\n",
            "epoch: 74, train acc: 0.97458, val acc: 0.9508\n",
            "overfitting warning: 0\n",
            "epoch: 75, train acc: 0.97514, val acc: 0.948\n",
            "overfitting warning: 1\n",
            "epoch: 76, train acc: 0.98086, val acc: 0.9561\n",
            "overfitting warning: 2\n",
            "epoch: 77, train acc: 0.98636, val acc: 0.9585\n",
            "epoch: 78, train acc: 0.98928, val acc: 0.9603\n",
            "epoch: 79, train acc: 0.98804, val acc: 0.9605\n",
            "epoch: 80, train acc: 0.9923, val acc: 0.9626\n",
            "epoch: 81, train acc: 0.99406, val acc: 0.964\n",
            "epoch: 82, train acc: 0.99472, val acc: 0.9637\n",
            "overfitting warning: 0\n",
            "epoch: 83, train acc: 0.99488, val acc: 0.9621\n",
            "overfitting warning: 1\n",
            "epoch: 84, train acc: 0.99204, val acc: 0.9605\n",
            "overfitting warning: 2\n",
            "epoch: 85, train acc: 0.99674, val acc: 0.9642\n",
            "epoch: 86, train acc: 0.99524, val acc: 0.9632\n",
            "overfitting warning: 0\n",
            "epoch: 87, train acc: 0.9948, val acc: 0.9622\n",
            "overfitting warning: 1\n",
            "epoch: 88, train acc: 0.99776, val acc: 0.9647\n",
            "epoch: 89, train acc: 0.99692, val acc: 0.9641\n",
            "overfitting warning: 0\n",
            "epoch: 90, train acc: 0.99744, val acc: 0.966\n",
            "epoch: 91, train acc: 0.9966, val acc: 0.9637\n",
            "overfitting warning: 0\n",
            "epoch: 92, train acc: 0.99896, val acc: 0.9672\n",
            "epoch: 93, train acc: 0.99824, val acc: 0.9666\n",
            "overfitting warning: 0\n",
            "epoch: 94, train acc: 0.9982, val acc: 0.9662\n",
            "overfitting warning: 1\n",
            "epoch: 95, train acc: 0.9998, val acc: 0.9667\n",
            "overfitting warning: 2\n",
            "epoch: 96, train acc: 0.99944, val acc: 0.9669\n",
            "overfitting warning: 3\n",
            "epoch: 97, train acc: 0.99946, val acc: 0.9673\n",
            "epoch: 98, train acc: 0.99946, val acc: 0.967\n",
            "overfitting warning: 0\n",
            "epoch: 99, train acc: 0.99946, val acc: 0.9663\n",
            "overfitting warning: 1\n",
            "epoch: 100, train acc: 0.99978, val acc: 0.9671\n",
            "overfitting warning: 2\n",
            "epoch: 101, train acc: 0.99988, val acc: 0.9671\n",
            "overfitting warning: 3\n",
            "epoch: 102, train acc: 0.99992, val acc: 0.9678\n",
            "epoch: 103, train acc: 0.99996, val acc: 0.968\n",
            "epoch: 104, train acc: 1.0, val acc: 0.9678\n",
            "overfitting warning: 0\n",
            "epoch: 105, train acc: 1.0, val acc: 0.9678\n",
            "overfitting warning: 1\n",
            "epoch: 106, train acc: 1.0, val acc: 0.9679\n",
            "overfitting warning: 2\n",
            "epoch: 107, train acc: 1.0, val acc: 0.9679\n",
            "overfitting warning: 3\n",
            "epoch: 108, train acc: 1.0, val acc: 0.9681\n",
            "epoch: 109, train acc: 1.0, val acc: 0.968\n",
            "overfitting warning: 0\n",
            "epoch: 110, train acc: 1.0, val acc: 0.9678\n",
            "overfitting warning: 1\n",
            "epoch: 111, train acc: 1.0, val acc: 0.9678\n",
            "overfitting warning: 2\n",
            "epoch: 112, train acc: 1.0, val acc: 0.9677\n",
            "overfitting warning: 3\n",
            "epoch: 113, train acc: 1.0, val acc: 0.9677\n",
            "overfitting warning: 4\n",
            "epoch: 114, train acc: 1.0, val acc: 0.9678\n",
            "early stopped on 114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTboa2XpP-P3"
      },
      "source": [
        "## **Test with the best epoch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up5W8iaAP9Qd",
        "outputId": "fe4445f6-46b4-48d7-e590-5cc3840593d0"
      },
      "source": [
        "# Start testing\r\n",
        "with tf.Session() as sess:\r\n",
        "    # Restore variables from disk.\r\n",
        "    saver.restore(sess, \"model/model.ckpt\")\r\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\r\n",
        "    # Calculate accuracy\r\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n",
        "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0})) # test 할 때도 모든 노드 사용(keep_prob)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
            "[Test Accuracy] : 0.9628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kYwTbUpQ7nD"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}