{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Implementation with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 35 # node 수\n",
    "lr = 0.01 # learning rate\n",
    "epochs = 1000\n",
    "\n",
    "string = \"hello pytorch. how long can a rnn cell remember?\"\n",
    "chars = \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n",
    "char_list = [i for i in chars]\n",
    "n_letters = len(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장이 input으로 들어왔을 때 이것을 연산 가능한 one-hot-vector로 바꾸는 함수\n",
    "def string_to_onehot(string):\n",
    "    start = np.zeros(shape = len(char_list), dtype = int)\n",
    "    end = np.zeros(shape = len(char_list), dtype = int)\n",
    "    start[-2] = 1\n",
    "    end[-1] = 1\n",
    "    for i in string:\n",
    "        idx = char_list.index(i)\n",
    "        zero = np.zeros(shape = n_letters, dtype = int)\n",
    "        zero[idx] = 1\n",
    "        start = np.vstack([start, zero])\n",
    "    output = np.vstack([start, end])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-vector를 다시 문자로 바꾸는 함수\n",
    "# 토치 텐서를 입력으로 받아 이를 넘파이 배열로 변환하고, 거기서 1인 지점을 인덱스로 잡아 char_list에서 뽑아낸다\n",
    "def onehot_to_word(onehot_1):\n",
    "    onehot = torch.Tensor.numpy(onehot_1)\n",
    "    return char_list[onehot.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN class 정의\n",
    "one-hot-vector로 변환한 단어 하나를 입력값으로 받고 hidden layer 하나를 통과시켜 결과값을 내는 구조.  \n",
    "입력값이 들어오면 이전 시간의 은닉층 값과의 조합으로 새로운 은닉층 값을 생성하고, 은닉층에서 결과값을 내는 부분의 연산을 한 번 더 통과해 결과값이 나오게 된다.  \n",
    "그리고 이전 시간의 은닉층 연산값이 없는 초기의 은닉층 값은 0으로 초기화해야하기 때문에 init_hidden이라는 함수를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.act_fn = nn.Tanh()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.act_fn(self.i2h(input) + self.h2h(hidden))\n",
    "        output = self.i2o(hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    \n",
    "rnn = RNN(n_letters, n_hidden, n_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss func, optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()  #L2 loss func\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "1. 학습하고자 하는 문장을 one-ot-vector로 변환한 numpy 배열을 토치 텐서 형태로 바꾼다. 이때 자료형은 연산에서 기본적으로 사용되는 torch.FloatTensor로 지정   \n",
    "2. 앞서 만든 함수대로 start_token + 문장 + end_token 의 구조를 가진 매트릭스가 생성된다. 학습할 때 시작 토큰이 들어오면 결과로 p가, p가 들어오면 y, y가 들어오면 t가 나오게 된다.  \n",
    "3. one-hot-vector는 문장에 있는 단어 순서대로 배열되어 있으므로 j번째 인덱스에 해당하는 값이 입력으로 들어오면 j+1번째 인덱스에 해당하는 값이 target이 되면 된다.  \n",
    "4. 문장 전체를 학습하는 과정은 epochs에 지정한 만큼 반복. 이때 내부적으로 입력값과 목표값의 차이를 계산해 문장 전체에 대한손실을 계산해야 한다. 그런데 문장에 대해 학습할 때 매번 loss를 계산해야하므로 total_loss는 0으로 초기화.  \n",
    "5. 학습을 시작하려면 rnn 은닉층의 초기값을 지정해야하므로 rnn.init_hidden()함수를 통해 0으로 초기화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epohc : 0 loss : 2.321022\n",
      "epohc : 100 loss : 0.076385\n",
      "epohc : 200 loss : 0.036367\n",
      "epohc : 300 loss : 0.014058\n",
      "epohc : 400 loss : 0.009746\n",
      "epohc : 500 loss : 0.007281\n",
      "epohc : 600 loss : 0.019789\n",
      "epohc : 700 loss : 0.007365\n",
      "epohc : 800 loss : 0.003761\n",
      "epohc : 900 loss : 0.006805\n"
     ]
    }
   ],
   "source": [
    "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n",
    "\n",
    "for i in range(epochs):\n",
    "    rnn.zero_grad()\n",
    "    total_loss = 0\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    for j in range(one_hot.size()[0]-1):\n",
    "        input_ = one_hot[j:j+1,:]\n",
    "        target = one_hot[j+1]\n",
    "        \n",
    "        output, hidden = rnn.forward(input_, hidden)\n",
    "        loss = loss_func(output.view(-1), target.view(-1))\n",
    "        total_loss += loss\n",
    "        input_ = output\n",
    "        \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"epohc : %d loss : %f\"%(i, total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 예제이므로 학습에 사용한 문장을 그대로 테스트에도 사용.  \n",
    "학습할 때는 단어 하나하나를 다 입력으로 넣어주었지만, 테스트 시에는 첫 글자만 입력으로 전달하고 그 다음부터는 모델에서 나온 결과값을 새로운 입력으로 전달해 첫 글자만으로 전체 문장을 생성해내는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello eceoececbcroa a.ro c cn  c c   onc c oh  c\n"
     ]
    }
   ],
   "source": [
    "start = torch.zeros(1, len(char_list))\n",
    "start[:, -2] = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden = rnn.init_hidden()\n",
    "    input_ = start\n",
    "    output_string = \"\"\n",
    "    for i in range(len(string)):\n",
    "        output, hidden = rnn.forward(input_, hidden)\n",
    "        output_string += onehot_to_word(output.data)\n",
    "        input_ = output\n",
    "        \n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
