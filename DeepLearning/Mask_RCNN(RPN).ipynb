{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask-RCNN(RPN)",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNj7ZAoKvOuWBeel4AUayh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanghee-Lee/Mask-RCNN_TF/blob/master/Mask_RCNN(RPN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QCILgjXGPLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "!pip install xmltodict\n",
        "import xmltodict\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras import layers as layers\n",
        "from keras import models as models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QppSjIWIKhCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#download VOC data and extract .tar file\n",
        "!mkdir train\n",
        "!mkdir test\n",
        "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar -P train/\n",
        "!wget https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip -P train/\n",
        "!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar -P test/\n",
        "!tar -xf test/VOCtest_06-Nov-2007.tar -C test/\n",
        "!tar -xf train/VOCtrainval_06-Nov-2007.tar -C train/\n",
        "!unzip train/PASCAL_VOC.zip -d train/\n",
        "!rm -rf train/PASCAL_VOC.zip train/VOCtrainval_06-Nov-2007.tar\n",
        "!rm -rf test/VOCtest_06-Nov-2007.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JIp9A0LHAwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzYVT9h2Gjo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_images_path     = os.getcwd()+'/train/VOCdevkit/VOC2007/JPEGImages'\n",
        "data_annotation_path = os.getcwd()+'/train/VOCdevkit/VOC2007/Annotations'\n",
        "img_min=800\n",
        "img_max=1024\n",
        "image_height = 1024\n",
        "image_width  = 1024\n",
        "image_depth  = 3        # RGB\n",
        "rpn_kernel_size = 3     # 3x3\n",
        "subsampled_ratio = [4, 8, 16, 32, 64]    # Pooling 3 times\n",
        "anchor_sizes = [32,64,128, 256, 512]      # Using [128, 256, 512] sizes in 1000x600 img size / Used [32, 64, 128] sizes in 224x224\n",
        "anchor_aspect_ratio = [[1,1],[1/math.sqrt(2),math.sqrt(2)],[math.sqrt(2),1/math.sqrt(2)]]\n",
        "num_anchors_in_box = len(anchor_aspect_ratio)\n",
        "neg_threshold = 0.3\n",
        "pos_threshold = 0.7\n",
        "nms_threshold = 0.7\n",
        "anchor_sampling_amount = 128        # 128 for each positive, negative sampling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy2n8V-HG2CN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])    \n",
        "#list_images=list_images[:2000]\n",
        "total_images = len(list_images)\n",
        "print(total_images)\n",
        "list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) \n",
        "#list_annotations = list_annotations[:2000]\n",
        "\n",
        "# evaluating data consistency between images and annotations\n",
        "t1=[]\n",
        "t2=[]\n",
        "for i in range(len(list_images)) :\n",
        "    t1.append(list_images[i][-11:-4])\n",
        "for i in range(len(list_annotations)) :\n",
        "    t2.append(list_annotations[i][-11:-4])\n",
        "\n",
        "for i in range(len(list_annotations)) :\n",
        "    if t2[i] not in t1 :\n",
        "        print(list_annotations[i])\n",
        "\n",
        "print(len(list_annotations))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXvffpMVG3OC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classes(xml_files=list_annotations):\n",
        "    '''\n",
        "    Input : dataset's annotations\n",
        "    parsing xml data to get objects label from images\n",
        "    Output : class label\n",
        "    \n",
        "    '''\n",
        "    classes = []\n",
        "    \n",
        "    for file in xml_files: \n",
        "\n",
        "        f = open(file)\n",
        "        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n",
        "        # annotation - object - [obj1, obj2 , ...]\n",
        "        try: \n",
        "            \n",
        "            for obj in doc['annotation']['object']:\n",
        "                classes.append(obj['name'].lower()) \n",
        "        # annotation - object\n",
        "        except TypeError as e: \n",
        "            classes.append(doc['annotation']['object']['name'].lower()) \n",
        "\n",
        "        f.close()\n",
        "\n",
        "    classes = list(set(classes)) \n",
        "    classes.sort()\n",
        "\n",
        "    return classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W2F4O8XG5ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = get_classes(list_annotations)\n",
        "print(classes)\n",
        "num_of_class = len(classes)\n",
        "print(num_of_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDwvH-y8G69Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_labels_from_xml(xml_file_path, num_of_class = num_of_class):\n",
        "    '''\n",
        "    Input : 1 xml file\n",
        "    Get class label, gt box coordinates.\n",
        "    Because images are resized to 224x224, coordinates also need to be resized.\n",
        "    Output: Existing class label, ground truth box coordinates\n",
        "    '''\n",
        "\n",
        "    f = open(xml_file_path)\n",
        "    doc = xmltodict.parse(f.read()) \n",
        "\n",
        "\n",
        "    ori_img_height = float(doc['annotation']['size']['height'])\n",
        "    ori_img_width  = float(doc['annotation']['size']['width'])\n",
        "\n",
        "\n",
        "    class_label = [] \n",
        "    bbox_label  = [] \n",
        "\n",
        "    # multi-objects in image\n",
        "    try:\n",
        "        for each_obj in doc['annotation']['object']:\n",
        "            obj_class = each_obj['name'].lower() \n",
        "            # Get bounding box coordinates\n",
        "            x_min = float(each_obj['bndbox']['xmin']) # top left x-axis coordinate.\n",
        "            x_max = float(each_obj['bndbox']['xmax']) # bottom right x-axis coordinate.\n",
        "            y_min = float(each_obj['bndbox']['ymin']) # top left y-axis coordinate.\n",
        "            y_max = float(each_obj['bndbox']['ymax']) # bottom right y-axis coordinate.\n",
        "\n",
        "            # Images resized to 224x224. So resize the coordinates\n",
        "            x_min = float((image_width/ori_img_width)*x_min)\n",
        "            y_min = float((image_height/ori_img_height)*y_min)\n",
        "            x_max = float((image_width/ori_img_width)*x_max)\n",
        "            y_max = float((image_height/ori_img_height)*y_max)\n",
        "\n",
        "            generated_box_info = [x_min, y_min, x_max, y_max]       # [top-left, bottom-right]\n",
        "\n",
        "            index = classes.index(obj_class) \n",
        "\n",
        "            class_label.append(index)\n",
        "            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
        "\n",
        "    # single-object in image\n",
        "    except TypeError as e : \n",
        "\n",
        "        obj_class = doc['annotation']['object']['name']\n",
        "        x_min = float(doc['annotation']['object']['bndbox']['xmin']) \n",
        "        x_max = float(doc['annotation']['object']['bndbox']['xmax']) \n",
        "        y_min = float(doc['annotation']['object']['bndbox']['ymin']) \n",
        "        y_max = float(doc['annotation']['object']['bndbox']['ymax']) \n",
        "\n",
        "        x_min = float((image_width/ori_img_width)*x_min)\n",
        "        y_min = float((image_height/ori_img_height)*y_min)\n",
        "        x_max = float((image_width/ori_img_width)*x_max)\n",
        "        y_max = float((image_height/ori_img_height)*y_max)\n",
        "\n",
        "        generated_box_info = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "        index = classes.index(obj_class) \n",
        "\n",
        "\n",
        "        class_label.append(index)\n",
        "        bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n",
        "\n",
        "\n",
        "    return class_label, np.asarray(bbox_label)\n",
        "xml_file_path=list_annotations[3]\n",
        "\n",
        "im = cv2.imread(list_images[3])\n",
        "cv2_imshow(im)\n",
        "\n",
        "class_label, bbox_label = get_labels_from_xml(xml_file_path, num_of_class = num_of_class)\n",
        "print(class_label)\n",
        "print(bbox_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvoLY11aG8wC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_anchors(one_anchor_size, one_subsampled_ratio,\n",
        "                     rpn_kernel_size=rpn_kernel_size, anchor_aspect_ratio=anchor_aspect_ratio):\n",
        "\n",
        "    '''\n",
        "    Input : subsample_ratio (=Pooled ratio)\n",
        "    * 1 anchor size, subsampled ratio are correnpondent to 3 anchor ratio *\n",
        "    generate anchor in feature map. Then project it to original image.\n",
        "    Output : list of anchors (x,y,w,h) and anchor_boolean (ignore anchor if value equals 0)\n",
        "\n",
        "    '''\n",
        "\n",
        "    list_of_anchors = []\n",
        "    anchor_booleans = [] #This is to keep track of an anchor's status. Anchors that are out of boundary are meant to be ignored.\n",
        "\n",
        "    starting_center = divmod(rpn_kernel_size, 2)[0] # rpn kernel's starting center in feature map\n",
        "    \n",
        "    anchor_center = [starting_center - 1,starting_center] # -1 on the x-coor because the increment comes first in the while loop\n",
        "    \n",
        "    subsampled_height = image_height/one_subsampled_ratio       # \n",
        "    subsampled_width = image_width/one_subsampled_ratio         # \n",
        "    \n",
        "    while (anchor_center != [subsampled_width, subsampled_heigh]):  # != [26, 26] -> [N, N]\n",
        "\n",
        "        anchor_center[0] += 1 #Increment x-axis\n",
        "\n",
        "        #If sliding window reached last center, increase y-axis\n",
        "        if anchor_center[0] > subsampled_width - (1 + starting_center):\n",
        "            anchor_center[1] += 1\n",
        "            anchor_center[0] = starting_center\n",
        "\n",
        "        #anchors are referenced to the original image. \n",
        "        #Therefore, multiply downsampling ratio to obtain input image's center \n",
        "        anchor_center_on_image = [anchor_center[0]*one_subsampled_ratio, anchor_center[1]*one_subsampled_ratio]\n",
        "\n",
        "        for size in [one_anchor_size]:\n",
        "            print(size)\n",
        "            for a_ratio in anchor_aspect_ratio:\n",
        "                # [x,y,w,h] --> [y, x, h, w]\n",
        "                anchor_info = [anchor_center_on_image[1], anchor_center_on_image[0], size*a_ratio[1], size*a_ratio[0]]\n",
        "                print(anchor_info)\n",
        "                # check whether anchor crosses the boundary of the image or not\n",
        "                \n",
        "                if (anchor_info[0] - anchor_info[2]/2 < 0 or anchor_info[0] + anchor_info[2]/2 > image_width or \n",
        "                                        anchor_info[1] - anchor_info[3]/2 < 0 or anchor_info[1] + anchor_info[3]/2 > image_height) :\n",
        "\n",
        "                    anchor_booleans.append([0.0])       # if anchor crosses boundary, anchor_booleans=0\n",
        "\n",
        "                else:\n",
        "\n",
        "                    anchor_booleans.append([1.0])\n",
        "\n",
        "                list_of_anchors.append(anchor_info)\n",
        "    \n",
        "    return list_of_anchors, anchor_booleans\n",
        "\n",
        "def generate_pyramid_anchors(anchor_sizes=anchor_sizes, subsampled_ratio=subsampled_ratio,\n",
        "                             rpn_kernel_size=rpn_kernel_size, anchor_aspect_ratio=anchor_aspect_ratio):\n",
        "    list_of_anchors, anchor_booleans=[], []\n",
        "    for i in range(len(anchor_sizes)) :\n",
        "        anchors, bools = generate_anchors(one_subsampled_ratio=subsampled_ratio[i],\n",
        "                                          one_anchor_size=anchor_sizes[i])\n",
        "        list_of_anchors.append(anchors)\n",
        "        anchor_booleans.append(bools)\n",
        "    return np.concatenate(list_of_anchors, axis=0), np.concatenate(anchor_booleans)    # return all of anchors generated from pyramid feature network / shape : N*3\n",
        "def normalize_box(anchors) :\n",
        "    width=anchors[:,3]-anchors[:,1]\n",
        "    height=anchors[:,2] - anchors[:,0]\n",
        "    scale=np.array([h-1, w-1, h-1, w-1])\n",
        "    shift=np.array([0, 0, 1, 1])\n",
        "    return np.divide((anchors-shift), scale).astype(np.float32)\n",
        "\n",
        "generate_pyramid_anchors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy7lR6pLnIFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "a=np.array([1, 2, 3])\n",
        "b=np.array([1, 2, 3])\n",
        "c=[]\n",
        "c.append(a)\n",
        "c.append(b)\n",
        "print(np.concatenate(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joStnOmpHA1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_label(class_labels, ground_truth_boxes, anchors, anchor_booleans, num_class=num_of_class,\n",
        "                    neg_anchor_thresh = neg_threshold, pos_anchor_thresh = pos_threshold):\n",
        "    '''\n",
        "    Input  : classes, ground truth box (top-left, bottom-right), all of anchors, anchor booleans.\n",
        "    Compute IoU to get positive, negative samples.\n",
        "    if IoU > 0.7, positive / IoU < 0.3, negative / Otherwise, ignore\n",
        "    Output : anchor booleans (to know which anchor to ignore), objectness label, regression coordinate in one image\n",
        "    '''\n",
        "\n",
        "\n",
        "    number_of_anchors = len(anchors) #Get the total number of anchors.\n",
        "\n",
        "    anchor_boolean_array   = np.reshape(np.asarray(anchor_booleans),(number_of_anchors, 1))\n",
        "    \n",
        "    # IoU is more than threshold or not.\n",
        "    objectness_label_array = np.zeros((number_of_anchors, 2), dtype=np.float32)\n",
        "    # delta(x, y, w, h)\n",
        "    box_regression_array   = np.zeros((number_of_anchors, 4), dtype=np.float32)\n",
        "    # belongs to which object for every anchor\n",
        "    class_array            = np.zeros((number_of_anchors, num_class), dtype=np.float32)\n",
        "    \n",
        "    for j in range(ground_truth_boxes.shape[0]):\n",
        "\n",
        "        #Get the ground truth box's coordinates.\n",
        "        gt_box_top_left_x = ground_truth_boxes[j][0]\n",
        "        gt_box_top_left_y = ground_truth_boxes[j][1]\n",
        "        gt_box_btm_rght_x = ground_truth_boxes[j][2]\n",
        "        gt_box_btm_rght_y = ground_truth_boxes[j][3]\n",
        "\n",
        "        #Calculate the area of the original bounding box.1 is added since the index starts from 0 not 1.\n",
        "        gt_box_area = (gt_box_btm_rght_x - gt_box_top_left_x + 1)*(gt_box_btm_rght_y - gt_box_top_left_y + 1)\n",
        "\n",
        "    \n",
        "        for i in range(number_of_anchors):\n",
        "\n",
        "            ######### Compute IoU #########\n",
        "\n",
        "            # Check if the anchor should be ignored or not. If it is to be ignored, it crosses boundary of image.\n",
        "            if int(anchor_boolean_array[i][0]) == 0:\n",
        "\n",
        "                continue\n",
        "\n",
        "            anchor = anchors[i] #Select the i-th anchor [x,y,w,h]\n",
        "\n",
        "            #anchors are in [x,y,w,h] format, convert them to the [top-left-x, top-left-y, btm-right-x, btm-right-y]\n",
        "            anchor_top_left_x = anchor[0] - anchor[2]/2\n",
        "            anchor_top_left_y = anchor[1] - anchor[3]/2\n",
        "            anchor_btm_rght_x = anchor[0] + anchor[2]/2\n",
        "            anchor_btm_rght_y = anchor[1] + anchor[3]/2\n",
        "\n",
        "            # Get the area of the bounding box.\n",
        "            anchor_box_area = (anchor_btm_rght_x - anchor_top_left_x + 1)*(anchor_btm_rght_y - anchor_top_left_y + 1)\n",
        "\n",
        "            # Determine the intersection rectangle.\n",
        "            int_rect_top_left_x = max(gt_box_top_left_x, anchor_top_left_x)\n",
        "            int_rect_top_left_y = max(gt_box_top_left_y, anchor_top_left_y)\n",
        "            int_rect_btm_rght_x = min(gt_box_btm_rght_x, anchor_btm_rght_x)\n",
        "            int_rect_btm_rght_y = min(gt_box_btm_rght_y, anchor_btm_rght_y)\n",
        "\n",
        "            # if the boxes do not intersect, difference = 0\n",
        "            int_rect_area = max(0, int_rect_btm_rght_x - int_rect_top_left_x + 1)*max(0, int_rect_btm_rght_y - int_rect_top_left_y)\n",
        "\n",
        "            # Calculate the IoU\n",
        "            intersect_over_union = float(int_rect_area / (gt_box_area + anchor_box_area - int_rect_area))\n",
        "            \n",
        "            # Positive\n",
        "            if intersect_over_union >= pos_anchor_thresh:\n",
        "\n",
        "                objectness_label_array[i][0] = 1.0 \n",
        "                objectness_label_array[i][1] = 0.0 \n",
        "                \n",
        "                #get the class label\n",
        "                class_label = class_labels[j]\n",
        "                class_array[i][int(class_label)] = 1.0 #Denote the label of the class in the array.\n",
        "                \n",
        "                #Get the ground-truth box's [x,y,w,h]\n",
        "                gt_box_center_x = ground_truth_boxes[j][0] + ground_truth_boxes[j][2]/2\n",
        "                gt_box_center_y = ground_truth_boxes[j][1] + ground_truth_boxes[j][3]/2\n",
        "                gt_box_width    = ground_truth_boxes[j][2] - ground_truth_boxes[j][0]\n",
        "                gt_box_height   = ground_truth_boxes[j][3] - ground_truth_boxes[j][1]\n",
        "\n",
        "                #Regression loss / weight\n",
        "                delta_x = (gt_box_center_x - anchor[0])/anchor[2]\n",
        "                delta_y = (gt_box_center_y - anchor[1])/anchor[3]\n",
        "                delta_w = math.log(gt_box_width/anchor[2])\n",
        "                delta_h = math.log(gt_box_height/anchor[3])\n",
        "\n",
        "                box_regression_array[i][0] = delta_x\n",
        "                box_regression_array[i][1] = delta_y\n",
        "                box_regression_array[i][2] = delta_w\n",
        "                box_regression_array[i][3] = delta_h\n",
        "\n",
        "            if intersect_over_union <= neg_anchor_thresh:\n",
        "                if int(objectness_label_array[i][0]) == 0:\n",
        "                    objectness_label_array[i][1] = 1.0\n",
        "\n",
        "            if intersect_over_union > neg_anchor_thresh and intersect_over_union < pos_anchor_thresh:\n",
        "                if int(objectness_label_array[i][0]) == 0 and int(objectness_label_array[i][1]) == 0:\n",
        "                    anchor_boolean_array[i][0] = 0.0 # ignore this anchor\n",
        "\n",
        "\n",
        "    return anchor_boolean_array, objectness_label_array, box_regression_array, class_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0IThavQHEfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def anchor_sampling(anchor_booleans, objectness_label, anchor_sampling_amount=anchor_sampling_amount):\n",
        "\n",
        "    '''\n",
        "    Input : anchor booleans and objectness label\n",
        "    fixed amount of negative anchors and positive anchors for training. \n",
        "    If we use all the neg and pos anchors, model will overfit on the negative samples.\n",
        "    Output: Updated anchor booleans. \n",
        "    '''\n",
        "\n",
        "    positive_count = 0\n",
        "    negative_count = 0\n",
        "    \n",
        "    for i in range(objectness_label.shape[0]):\n",
        "        if int(objectness_label[i][0]) == 1: #If the anchor is positive\n",
        "\n",
        "            if positive_count > anchor_sampling_amount: #If the positive anchors are more than the threshold amount, set the anchor boolean to 0.\n",
        "\n",
        "                anchor_booleans[i][0] = 0.0\n",
        "\n",
        "            positive_count += 1\n",
        "\n",
        "        if int(objectness_label[i][1]) == 1: #If the anchor is negatively labelled.\n",
        "            if negative_count > anchor_sampling_amount: #If the negative anchors are more than the threshold amount, set the boolean to 0.\n",
        "\n",
        "                anchor_booleans[i][0] = 0.0\n",
        "\n",
        "            negative_count += 1\n",
        "    \n",
        "    return anchor_booleans\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvyt69uEHHU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_dataset(first_index, last_index, anchors, anchor_booleans):\n",
        "        '''\n",
        "        Input : starting index and final index of the dataset to be generated.\n",
        "        Output: Anchor booleans, Objectness Label and Regression Label in batches.\n",
        "        '''\n",
        "        num_of_anchors = len(anchors)\n",
        "        \n",
        "        batch_anchor_booleans   = []\n",
        "        batch_objectness_array  = []\n",
        "        batch_regression_array  = []\n",
        "        batch_class_label_array = []\n",
        "\n",
        "        for i in range(first_index, last_index):\n",
        "\n",
        "            #Get the true labels and the ground truth boxes [x,y,w,h] for every file.\n",
        "            true_labels, ground_truth_boxes = get_labels_from_xml(xml_file_path=list_annotations[i])\n",
        "\n",
        "            # generate_labels for specified batches\n",
        "            anchor_bools, objectness_label_array, box_regression_array, class_array = generate_label(true_labels, ground_truth_boxes, \n",
        "                                                                                                        anchors, anchor_booleans)\n",
        "            #ggenerate_label(class_labels, ground_truth_boxes, anchors, anchor_booleans, num_class=num_of_class,\n",
        "            #        neg_anchor_thresh = neg_threshold, pos_anchor_thresh = pos_threshold)\n",
        "\n",
        "            # get the updated anchor bools based on the fixed number of sample\n",
        "            anchor_bools = anchor_sampling(anchor_bools, objectness_label_array)\n",
        "            \n",
        "            batch_anchor_booleans.append(anchor_bools)\n",
        "            batch_objectness_array.append(objectness_label_array)\n",
        "            batch_regression_array.append(box_regression_array)\n",
        "            batch_class_label_array.append(class_array)\n",
        "\n",
        "        batch_anchor_booleans   = np.reshape(np.asarray(batch_anchor_booleans), (-1,num_of_anchors))            # (1, 6084, 1) -> (1, 6084)\n",
        "        \n",
        "        batch_objectness_array  = np.asarray(batch_objectness_array)\n",
        "        batch_regression_array  = np.asarray(batch_regression_array)\n",
        "        batch_class_label_array = np.asarray(batch_class_label_array)\n",
        "\n",
        "        return (batch_anchor_booleans, batch_objectness_array, batch_regression_array, batch_class_label_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqQCGHU5HJEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_images(first_index, last_index):\n",
        "    '''\n",
        "    Read the image files, then resize.\n",
        "    Input : first and last index.\n",
        "    Output: numpy array of images.\n",
        "    '''\n",
        "    images_list = []\n",
        "    \n",
        "    for i in range(first_index, last_index):\n",
        "        \n",
        "        im = cv2.imread(list_images[i])\n",
        "        im = cv2.resize(im, (image_height, image_width))/255\n",
        "        \n",
        "        images_list.append(im)\n",
        "    \n",
        "    return np.asarray(images_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isM7-kOFHKym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anchors, an_bools = generate_anchors() #We only need to generate the anchors and the anchor booleans once.\n",
        "num_of_anchors = len(anchors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bo78X7oHM1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c,d = generate_dataset(0,1, anchors, an_bools)\n",
        "a.shape\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9X2aZphHN18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-5\n",
        "epoch = 100\n",
        "batch_size = 10\n",
        "!mkdir -p '/content/drive/My Drive/VOCdata/trained_weight'\n",
        "model_checkpoint = './drive/My Drive/VOCdata/trained_weight/model.ckpt'\n",
        "decay_steps = 10000\n",
        "decay_rate = 0.99\n",
        "lambda_value = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acSqLwUD-kLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth_func(t):\n",
        "    \n",
        "    t = tf.abs(t)\n",
        "    \n",
        "    comparison_tensor = tf.ones((num_of_anchors, 4))\n",
        "    smoothed = tf.where(tf.less(t, comparison_tensor), 0.5*tf.pow(t,2), t - 0.5)\n",
        "    \n",
        "    return smoothed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlqpqYl8-ltY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def smooth_L1(pred_box, truth_box):\n",
        "    \n",
        "    diff = pred_box - truth_box\n",
        "    \n",
        "    smoothed = tf.map_fn(smooth_func, diff)\n",
        "    \n",
        "    return smoothed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7eN-azgsAog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(input, filter_size, filter_num) :\n",
        "    y=layers.Conv2D(filter_num[0], (1, 1), strides=(1, 1), padding='valid', use_bias=True)(input)\n",
        "    y=layers.BatchNormalization(y)\n",
        "    y=layers.Activation('relu')(y)\n",
        "\n",
        "    y=layers.Conv2D(filter_num[1], (3, 3), strides=(1, 1), padding='same', use_bias=True)(y)\n",
        "    y=layers.BatchNormalization(y)\n",
        "    y=layers.Activation('relu')(y)\n",
        "\n",
        "    y=layers.Conv2D(filter_num[2], (1, 1), strides=(1, 1), padding='valid', use_bias=True)(y)\n",
        "    y=layers.BatchNormalization(y)\n",
        "    \n",
        "    y=layers.Add()([y, input])\n",
        "    y=layers.Activation('relu')(y)\n",
        "    return y\n",
        "\n",
        "def conv_block(input, filter_num, strides=(2, 2)) :\n",
        "    y=layers.Conv2D(filter_num[0], (1, 1), strides=strides, padding='valid', use_bias=True)(input)\n",
        "    y=layers.BatchNormalization(y)\n",
        "    y=layers.Activation('relu')(y)\n",
        "\n",
        "    y=layers.Conv2D(filter_num[1], (3, 3), strides=(1, 1), padding='same', use_bias=True)(y)\n",
        "    y=layers.BatchNormalization(y)\n",
        "    y=layers.Activation('relu')(y)\n",
        "\n",
        "    y=layers.Conv2D(filter_num[2], (1, 1), strides=(1, 1), padding='valid', use_bias=True)(y)\n",
        "    y=layers.BatchNormalization(y)\n",
        "    # add shortcut block for conv_block\n",
        "    shortcut=layers.Conv2D(filter_num[2], (1, 1), strides=strides, padding='valid', use_bias=True)(input)\n",
        "    shortcut=layers.BatchNormalization(shortcut)\n",
        "    \n",
        "    y=layers.Add()([y, shortcut])\n",
        "    y=layers.Activation('relu')(shortcut)\n",
        "    return y\n",
        "\n",
        "def resnet101(input) :\n",
        "    # stage 1\n",
        "    y=layers.ZeroPadding2D((3, 3))(input)\n",
        "    y=layers.Conv2D(64, (7, 7), strides=(2, 2), padding='valid', use_bias=True)(y)\n",
        "    y=layers.BatchNormalization(y)\n",
        "    y=layers.Activation('relu')(y)\n",
        "    F1=y=layers.MaxPooling2D((3, 3), strides(2, 2), padding='same')(y)\n",
        "\n",
        "    # stage 2 : In stage 2, use strides 1\n",
        "    y=conv_block(y, [64, 64, 256], strides=(1, 1))\n",
        "    y=identity_block(y, [64, 64, 256])    \n",
        "    F2=y=identity_block(y, [64, 64, 256])\n",
        "\n",
        "    # stage 3 \n",
        "    y=conv_block(y, [128, 128, 512])\n",
        "    y=identity_block(y, [128, 128, 256])    \n",
        "    y=identity_block(y, [128, 128, 256])\n",
        "    F3=y=identity_block(y, [128, 128, 256])\n",
        "\n",
        "    # stage 4 : In stage 4, there are 22 identity blocks\n",
        "    y=conv_block(y, [256, 256, 1024])\n",
        "    block_num=22\n",
        "    for i in range(block_num) :\n",
        "        y=identity_block(y, [256, 256, 1024])\n",
        "    F4=y\n",
        "    \n",
        "    # stage 5 \n",
        "    y=conv_block(y, [512, 512, 2048])\n",
        "    y=identity_block(y, [512, 512, 2048])\n",
        "    F5=y=identity_block(y, [512, 512, 2048])\n",
        "\n",
        "    return [F1, F2, F3, F4, F5]\n",
        "\n",
        "# Use 256 filters for FPN\n",
        "def fpn(input) :\n",
        "    P5=layers.Conv2d(256, (1, 1))(F5)\n",
        "    P5=layers.Conv2D(256, (3, 3), padding='same')(P5)\n",
        "    # Compute P4 by adding feature map4 and Upsampled result of P5\n",
        "    res4=layers.UpSammpling2D(size=(2, 2))(P5)\n",
        "    P4=layers.Add([res4, layers.Conv2D(256, (1, 1)(F4))])\n",
        "    P4=layers.Conv2D(256, (3, 3), padding='same')(P4)\n",
        "\n",
        "    res3=layers.UpSammpling2D(size=(2, 2))(P4)\n",
        "    P3=layers.Add([res4, layers.Conv2D(256, (1, 1)(F3))])\n",
        "    P3=layers.Conv2D(256, (3, 3), padding='same')(P3)\n",
        "\n",
        "    res2=layers.UpSammpling2D(size=(2, 2))(P3)\n",
        "    P2=layers.Add([res4, layers.Conv2D(256, (1, 1)(F2))])\n",
        "    P2=layers.Conv2D(256, (3, 3), padding='same')(P2)\n",
        "\n",
        "    #Issues : Why use (1, 1) for pool size and reason for not computing (3, 3) conv only in P6\n",
        "    P6=layers.MaxPooling2D(pool_size=(1, 1), strides=2)(P5)\n",
        "\n",
        "    return [P2, P3, P4, P5, P6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDEFMKcd-m9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rpn(input) :\n",
        "    # All of feature map's depth in FPN is 256\n",
        "    input_featuremap=layers.Input(shape=[None, None, 256])\n",
        "    shared4obj_and_bb=layers.Conv2d(512, (3, 3), padding='same', activation_fn='relu', strides=1)(input)\n",
        "    obj_conv = layers.Conv2d(6, (1, 1), padding='VALID', activation_fn='linear')(shared4obj_and_bb)         # None * None * 6\n",
        "    bb_conv = layers.Conv2d(12, (1, 1), padding='VALID', activation_fn='linear')(shared4obj_and_bb)          # None * None * 12\n",
        "\n",
        "    class_conv_reshape = tf.reshape(obj_conv, (-1, num_of_anchors, 2))          # 6084x2\n",
        "    anchor_conv_reshape = tf.reshape(bb_conv, (-1, num_of_anchors, 4))          # 6084x4\n",
        "\n",
        "    logits = tf.nn.softmax(class_conv_reshape)\n",
        "    \n",
        "    return models.Model([input_featuremap], [class_conv_reshape, logits, anchor_conv_reshape])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwyKXQFxeLBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_box_delta(boxes, delta) : \n",
        "    '''\n",
        "    Input : boxes [N, [y1, x1, y2, x2]]\n",
        "    boxes = anchor boxes (ranked top k of All anchors) \n",
        "    delta = correspondents to anchor boxes' delta (also ranked top k anchors' delta)\n",
        "\n",
        "    apply delta to boxes (* refined boxes' coordinates represent normalized coordinates)\n",
        "    '''\n",
        "    y1, x1, y2, x2 = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]\n",
        "    anchor_height = y2 - y1\n",
        "    anchor_width = x2 - x1\n",
        "    anchor_center_x = x1 + width*0.5\n",
        "    anchor_center_y = y1 + height*0.5\n",
        "    # apply delta to box\n",
        "    refined_center_x = delta[:,1] * width + anchor_center_x\n",
        "    refined_center_y = delta[:,0] * height + anchor_center_y\n",
        "    refined_width = tf.exp(delta[:,3]) * anchor_width\n",
        "    refined_height = tf.exp(delta[:,2]) * anchor_height\n",
        "\n",
        "    # compute refined coordinates : [y1, x1, y2, x2]\n",
        "    refined_x1=refined_center_x - 0.5*width\n",
        "    refined_x2=refined_center_x + 0.5*width\n",
        "    refined_y1=refined_center_y - 0.5*height\n",
        "    refined_y2=refined_center_y + 0.5*height\n",
        "\n",
        "    return tf.stack([refined_y1, refined_x1, refined_y2, refined_x2], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ql51sMdjPAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_iou_boxes(one_box, boxes, box_area, boxes_area) :\n",
        "    intersection_y1 = np.maximum(box[0], boxes[:, 0])\n",
        "    intersection_x1 = np.maximum(box[1], boxes[:, 1])\n",
        "    intersection_y2 = np.minimum(box[2], boxes[:, 2])\n",
        "    intersection_x2 = np.minimum(box[3], boxes[:, 3])\n",
        "\n",
        "    inter_area = np.maximum(intersection_x2-intersection_x1, 0) * np.maximum(intersection_y1-intersection_y2, 0)\n",
        "    union_area = box_area + boxes_area[:] - inter_area\n",
        "    iou = inter_area / union_area\n",
        "    return iou\n",
        "\n",
        "def non_max_suppression(boxes, scores) : \n",
        "    # scores : result from softmax of rpn's output(logits)\n",
        "    y1=boxes[:, 0]\n",
        "    x1=boxes[:, 1]\n",
        "    y2=boxes[:, 2]\n",
        "    x2=boxes[:, 3]\n",
        "    area=(y2-y1)*(x2-x1)\n",
        "\n",
        "    # Use fg score to sort\n",
        "    sorted_idx = scores.argsort()[::-1]         # return idx of sorted result\n",
        "    \n",
        "    result = []\n",
        "    while len(ixs) > 0:\n",
        "        # Pick top box and add its index to the list\n",
        "        i = sorted_idx[0]\n",
        "        pick.append(i)\n",
        "        # Compute IoU of the picked box with the rest\n",
        "        iou = compute_iou(boxes[i], boxes[ixs[1:]], area[i], area[ixs[1:]])\n",
        "        # Identify boxes with IoU over the 0.7(nms_threshold). \n",
        "        # This returns are correspondents to ixs[1:], so add 1 (ixs are correspondents to ixis[:]).\n",
        "        remove_ixs = np.where(iou > nms_threshold)[0] + 1\n",
        "        # Remove indices of the picked and overlapped boxes.\n",
        "        ixs = np.delete(ixs, remove_ixs)\n",
        "        ixs = np.delete(ixs, 0)\n",
        "    nms_roi = tf.gather(boxes, result)\n",
        "    return nms_roi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2JYUXi3-p0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss Function\n",
        "loss1 = 1/256*tf.reduce_sum(anch_bool*(tf.nn.softmax_cross_entropy_with_logits(labels=Y_obj, logits=class_conv_reshape)))       # positive(128) + negative(128)\n",
        "# (10, 6084, 2)\n",
        "loss2 = lambda_value*(1/128)*tf.reduce_sum((tf.reshape(Y_obj[:,:,0], (-1,num_of_anchors,1)))*smooth_L1(anchor_conv_reshape, Y_coor))\n",
        "\n",
        "total_loss = loss1 + loss2\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(decayed_lr).minimize(total_loss, global_step=global_step)\n",
        "\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "decayed_lr = tf.train.exponential_decay(learning_rate,\n",
        "                                            global_step, decay_steps,\n",
        "                                            decay_rate, staircase=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8GmIAcv-wKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "try:\n",
        "    saver.restore(sess, model_checkpoint)\n",
        "    print(\"Model has been loaded!\")\n",
        "    \n",
        "except:\n",
        "    \n",
        "    print(\"Model doens't exist!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3fc5DQ-ylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "def draw_a_rectangel_in_img(draw_obj, box, color, width):\n",
        "    '''\n",
        "    use draw lines to draw rectangle. since the draw_rectangle func can not modify the width of rectangle\n",
        "    :param draw_obj:\n",
        "    :param box: [x1, y1, x2, y2]\n",
        "    :return:\n",
        "    '''\n",
        "    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
        "    top_left, top_right = (x1, y1), (x2, y1)\n",
        "    bottom_left, bottom_right = (x1, y2), (x2, y2)\n",
        "\n",
        "    draw_obj.line(xy=[top_left, top_right],\n",
        "                  fill=color,\n",
        "                  width=width)\n",
        "    draw_obj.line(xy=[top_left, bottom_left],\n",
        "                  fill=color,\n",
        "                  width=width)\n",
        "    draw_obj.line(xy=[bottom_left, bottom_right],\n",
        "                  fill=color,\n",
        "                  width=width)\n",
        "    draw_obj.line(xy=[top_right, bottom_right],\n",
        "                  fill=color,\n",
        "                  width=width)\n",
        "PIXEL_MEAN = [123.68, 116.779, 103.939]\n",
        "def draw_boxes_with_label_and_scores(img_array, boxes):\n",
        "\n",
        "    img_array = img_array + np.array(PIXEL_MEAN)\n",
        "    img_array.astype(np.float32)\n",
        "    boxes = boxes.astype(np.int64)\n",
        "    #labels = labels.astype(np.int32)\n",
        "    img_array = np.array(img_array * 255 / np.max(img_array), dtype=np.uint8)\n",
        "    \n",
        "    img_obj = Image.fromarray(img_array)\n",
        "    #img_obj=img_array\n",
        "    print('zz')\n",
        "    raw_img_obj = img_obj.copy()\n",
        "    \n",
        "    draw_obj = ImageDraw.Draw(img_obj)\n",
        "    num_of_objs = 0\n",
        "    for box in boxes:\n",
        "         draw_a_rectangel_in_img(draw_obj, box, color='Coral', width=3)\n",
        "        \n",
        "\n",
        "    out_img_obj = Image.blend(raw_img_obj, img_obj, alpha=0.6)\n",
        "\n",
        "    return np.array(out_img_obj)\n",
        "'''\n",
        "#TRAINING \n",
        "\n",
        "for epoch_idx in range(epoch): #Each epoch.\n",
        "    \n",
        "    #Loop through the whole dataset in batches.\n",
        "    for start_idx in tqdm(range(0, total_images, batch_size)):\n",
        "        \n",
        "        end_idx = start_idx + batch_size\n",
        "        \n",
        "        if end_idx >= total_images : end_idx = total_images - 1 #In case the end index exceeded the dataset.\n",
        "            \n",
        "        images = read_images(start_idx, end_idx) #Read images.\n",
        "        \n",
        "        #Get the labels needed.\n",
        "        batch_anchor_booleans, batch_objectness_array, batch_regression_array, _ = \\\n",
        "                                                generate_dataset(start_idx,end_idx, anchors, an_bools)\n",
        "        print(batch_objectness_array.shape)\n",
        "        #Optimize the model.\n",
        "        anchor_reshape, _, theloss = sess.run([anchor_conv_reshape, optimizer, total_loss], feed_dict={X: images,\n",
        "                                                                  Y_obj:batch_objectness_array,\n",
        "                                                                  Y_coor: batch_regression_array,\n",
        "                                                                  anch_bool: batch_anchor_booleans})\n",
        "        \n",
        "        '''\n",
        "        img_array = cv2.imread(list_images[0])\n",
        "            #img_array = cv2.resize(img_array, (image_height, image_width))/255\n",
        "        img_array = np.array(img_array, np.float32) - np.array(PIXEL_MEAN)\n",
        "\n",
        "        anchor_booleans, objectness_array, regression_array, _ = \\\n",
        "                                                generate_dataset(0,1, anchors, an_bools)\n",
        "        img_array_tensor=read_images(0, 1)\n",
        "        anchor_reshape=sess.run([anchor_conv_reshape], feed_dict={X:img_array_tensor, Y_obj : objectness_array, Y_coor : regression_array, anch_bool : anchor_booleans})\n",
        "        print(anchor_reshape)\n",
        "        print(anchor_reshape[0])\n",
        "        \n",
        "        anchor_reshape=anchor_reshape[0]\n",
        "        boxes = np.array(\n",
        "        [[200, 200, 500, 500],\n",
        "            [300, 300, 400, 400],\n",
        "            [200, 200, 400, 400]]\n",
        "    )\n",
        "        anchor_reshape=np.array(anchor_reshape)\n",
        "        \n",
        "        im=draw_boxes_with_label_and_scores(img_array, np.array(anchor_reshape))\n",
        "        cv2_imshow(im)        \n",
        "        '''\n",
        "    #Save the model periodically.\n",
        "        saver.save(sess, model_checkpoint)\n",
        "    \n",
        "    print(\"Epoch : %d, Loss : %g\"%(epoch_idx, theloss))\n",
        "    img_array = cv2.imread(list_images[0])\n",
        "        #img_array = cv2.resize(img_array, (image_height, image_width))/255\n",
        "    img_array = np.array(img_array, np.float32) - np.array(PIXEL_MEAN)\n",
        "\n",
        "    anchor_booleans, objectness_array, regression_array, _ = \\\n",
        "                                            generate_dataset(0,1, anchors, an_bools)\n",
        "    img_array_tensor=read_images(0, 1)\n",
        "    anchor_reshape=sess.run([anchor_conv_reshape], feed_dict={X:img_array_tensor, Y_obj : objectness_array, Y_coor : regression_array, anch_bool : anchor_booleans})\n",
        "    print(anchor_reshape)\n",
        "    print(anchor_reshape[0])\n",
        "    anchor_reshape=anchor_reshape[0][0]\n",
        "    boxes = np.array(\n",
        "    [[200, 200, 500, 500],\n",
        "        [300, 300, 400, 400],\n",
        "        [200, 200, 400, 400]]\n",
        ")\n",
        "    im=draw_boxes_with_label_and_scores(img_array, np.array(anchor_reshape))\n",
        "    cv2_imshow(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBbxs5h5bvKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}