{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_MLP_TF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-P2TKVJ05bT"
      },
      "source": [
        "# **Tensorflow를 이용한 MNIST Multi Layer Perceptron 실습하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1XJefscc1JIZ",
        "outputId": "bf0b9821-d245-4a30-ff7e-c290a64afc8c"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "tf.__version__"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw8BV_gNdftv"
      },
      "source": [
        "## **MLP Architecture**\r\n",
        "input layer, hidden layer h1, h2, output layer logit. 10개의 output 노드가 10개의 softmax를 통해 prediction으로 변하고, 이 prediction을 y label과 비교를 함으로서 cross entropy를 가짐. 이 cross entropy를 최저로 가지는 것을 목적 함수로 해 adam optimizer를 돌려 architecture를 최적화 한다.\r\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\" height=\"450\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSb16qBXehGr"
      },
      "source": [
        "## **MNIST Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a8XxbIz1OxT"
      },
      "source": [
        "# mnist data load\r\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXseMkLqgAgo",
        "outputId": "4435dc91-4ec5-43e5-dd48-7389479de89a"
      },
      "source": [
        "print(x_train.shape)\r\n",
        "print(y_train.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Exk3QmRgtWg"
      },
      "source": [
        "train data는 **60000**개의 sample을 가진다.  \r\n",
        "test data는 **60000**개의 sample을 가진다.  \r\n",
        "모든 data는 **28*28** pixels\r\n",
        "  \r\n",
        "  MNIST는 사람이 손으로 쓴 숫자로 이루어지는 대형 데이터베이스다. 아래 이미지는 28*28 pexels로 구성되며 흰색과 검정색으로 이루어진 gray scale image [0 to 255]다.\r\n",
        "\r\n",
        "<p align=\"center\"><img src=\"https://mblogthumb-phinf.pstatic.net/MjAxOTAxMTZfMTg4/MDAxNTQ3NjQ2MzI5OTQ0.257wNc_ErINd8lMDTaor5ls3UP0T2lD1yV_KoC_Ik6Mg.OGV-dRDH3m9U48AhlEaptGI9D8tHsvP-OZbPYyofin4g.PNG.garden2040/Figure_1.png?type=w800\" height=\"300\"></p>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlB6x31Ji1nW"
      },
      "source": [
        "## **Split Train data into Train and Validation data**\r\n",
        "training 중 validation은 다음과 같은 장점을 가진다.  \r\n",
        "1. training 중 validation score를 이용해 hyper parameter를 수정할 수 있다.\r\n",
        "2. training score가 올라가는 동안 validation score(accuracy)가 변화가 없을 시 **early stopping**을 제공한다. (**overfitting** 방지)\r\n",
        "\r\n",
        "\r\n",
        "train data 60000개 중 10000개의 데이터를 validation data로 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVOmIoDAgMUg"
      },
      "source": [
        "x_val = x_train[50000:60000]\r\n",
        "x_train = x_train[0:50000]\r\n",
        "y_val = y_train[50000:60000]\r\n",
        "y_train = y_train[0:50000]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rb_nIAvlUZX"
      },
      "source": [
        "## **Reshape**\r\n",
        "\r\n",
        "모든 pixels을 hidden layer에 fully connected (완전 연결)하려면 (28,28)을 (28x28, 1)로 reshape 해야한다.  \r\n",
        "== row * col shape를 28x28개의 item이 있는 행렬로 flatten 해야 한다.\r\n",
        "\r\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\" height=\"420\"></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CahXAhGkslK",
        "outputId": "a5d1a647-06d1-407d-ef2b-00eed5278571"
      },
      "source": [
        "x_train = x_train.reshape(50000, 784)\r\n",
        "x_val = x_val.reshape(10000, 784)\r\n",
        "x_test = x_test.reshape(10000, 784)\r\n",
        "\r\n",
        "print(x_train.shape)\r\n",
        "print(x_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkN0lQ7fGr8c",
        "outputId": "bfe59a56-cbe5-4750-bb99-c00bde435fbb"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
              "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
              "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
              "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
              "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
              "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
              "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
              "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
              "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
              "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAj41fJhHWqs"
      },
      "source": [
        "## **Normalize data**\r\n",
        "정규화는 모든 입력 feature를 동일한 범위로 만들어 분산을 줄임으로써 학습 속도를 향상시키거나, 더 나은 성능을 돕는다.  \r\n",
        "MNIST 데이터 셋은 [0 to 255]의 값을 가지므로 정규화를 할 시 variance만 줄일 수 있다.  \r\n",
        "  \r\n",
        "MNIST는 MLP architecture로 표준화하는 것보다 정규화하는 것이 더 낫다. ReLU가 peed forward와 back propagation에서 모두 0을 다르게 다루기 때문. MNIST에서 [1 to 255]는 손으로 쓴 것을 의미하므로 아무 것도 쓰지 않은 0을 다르게 취급하는 것이 중요하다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZu6Ya6vG52p"
      },
      "source": [
        "x_train = x_train.astype('float32')\r\n",
        "x_val = x_val.astype('float32')\r\n",
        "x_test = x_test.astype('float32')\r\n",
        "\r\n",
        "# 255로 나누게 되면 [0 to 1]의 값을 가지게 됨\r\n",
        "gray_scale = 255 \r\n",
        "x_train /= gray_scale\r\n",
        "x_val /= gray_scale\r\n",
        "x_test /= gray_scale"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlLyNsfwKOdE"
      },
      "source": [
        "## **Label to One Hot Encoding Value**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxwwI13xJNFc"
      },
      "source": [
        "# class를 10으로 해 y값들을 모두 One Hot Encoding한다. (1~10까지의 숫자를 0~9번 노드로)\r\n",
        "num_classes = 10\r\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\r\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90O9Z3yMKppV",
        "outputId": "f4a1985e-bed1-457b-fc43-cd1d5b5174c4"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0t7hHhJK9ir"
      },
      "source": [
        "## **Tensorflow MLP Graph**\r\n",
        "\r\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\" height=\"450\"></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z0tcz4-K4N-"
      },
      "source": [
        "# tensorflow 2.x에서는 session을 정의하고 run을 수행하는 과정이 생략되고 바로 실행되는 형태가 되었으므로 아래와 같은 옵션을 추가해준다.\r\n",
        "tf.compat.v1.disable_eager_execution()\r\n",
        "# input으로 784개\r\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, 784])\r\n",
        "# [None, 10] : 총 몇 개의 sample이 들어올지 모르겠지만 10개의 숫자를 구분할 것이다.\r\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkxqxvKlL7r-"
      },
      "source": [
        "def mlp(x):\r\n",
        "  # hidden layer1\r\n",
        "  w1 = tf.Variable(tf.compat.v1.random_uniform([784, 256])) # 784개의 input을 받고, 256개의 node를 만든다.\r\n",
        "  b1 = tf.Variable(tf.zeros([256]))\r\n",
        "  h1 = tf.nn.relu(tf.matmul(x, w1) + b1) # activation func : ReLU\r\n",
        "  # hidden layer2\r\n",
        "  w2 = tf.Variable(tf.compat.v1.random_uniform([256, 128])) # 256개의 input을 받고, 128개의 node를 만든다.\r\n",
        "  b2 = tf.Variable(tf.zeros([128]))\r\n",
        "  h2 = tf.nn.relu(tf.matmul(h1, w2) + b2) # activation func : ReLU\r\n",
        "  # output layer\r\n",
        "  w3 = tf.Variable(tf.compat.v1.random_uniform([128, 10])) # 128개의 input을 받고, 10개의 node를 만든다.\r\n",
        "  b3 = tf.Variable(tf.zeros([10]))\r\n",
        "  logits = tf.matmul(h2, w3) + b3\r\n",
        "\r\n",
        "  return logits"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V0El7pZPICS"
      },
      "source": [
        "cross entropy를 하기 위해 softmax를 많이 사용한다. 이 logic value를 softmax에 넣어서 prediction을 받는다. 이 cross entropy를 minimize하는 것이 adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODT4NRlKN_GQ"
      },
      "source": [
        "logits = mlp(x)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9btcW1WePJrV"
      },
      "source": [
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXvOUMDOSvbh"
      },
      "source": [
        "train_op = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfOVIH8OVWyw",
        "outputId": "99c0056f-1b9c-40a6-90fa-67d24178f012"
      },
      "source": [
        "# initialize\r\n",
        "init = tf.global_variables_initializer()\r\n",
        "\r\n",
        "# train hyperparameters\r\n",
        "epoch_cnt = 30\r\n",
        "batch_size = 1000 # 1000개씩 넣으면서 architecture 최적화\r\n",
        "iteration = len(x_train) // batch_size # input이 50000개니까 총 50번의 iteration\r\n",
        "\r\n",
        "# Start training\r\n",
        "with tf.Session() as sess:\r\n",
        "    # Run the initializer\r\n",
        "    sess.run(init)\r\n",
        "    for epoch in range(epoch_cnt): # epoch 돌아감\r\n",
        "        avg_loss = 0.\r\n",
        "        start = 0; end = batch_size\r\n",
        "        \r\n",
        "        for i in range(iteration): # batch 돌아감\r\n",
        "            _, loss = sess.run([train_op, loss_op], feed_dict={x: x_train[start: end], y: y_train[start: end]})\r\n",
        "            start += batch_size; end += batch_size\r\n",
        "            # Compute average loss\r\n",
        "            avg_loss += loss / iteration\r\n",
        "            \r\n",
        "        # Validate model\r\n",
        "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\r\n",
        "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1)) # 몇개가 맞았는지\r\n",
        "        # Calculate accuracy\r\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n",
        "        cur_val_acc = accuracy.eval({x: x_val, y: y_val})\r\n",
        "        print(\"epoch: \"+str(epoch)+\", validation accuracy: \" + str(cur_val_acc) +', loss: '+str(avg_loss))\r\n",
        "    \r\n",
        "    # Test model\r\n",
        "    preds = tf.nn.softmax(logits)  # Apply softmax to logits\r\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\r\n",
        "    # Calculate accuracy\r\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n",
        "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test}))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, validation accuracy: 0.1923, loss: 9561.096894531249\n",
            "epoch: 1, validation accuracy: 0.7839, loss: 283.1611603164674\n",
            "epoch: 2, validation accuracy: 0.8761, loss: 13.110803556442264\n",
            "epoch: 3, validation accuracy: 0.8816, loss: 8.532276582717895\n",
            "epoch: 4, validation accuracy: 0.8855, loss: 6.908381538391112\n",
            "epoch: 5, validation accuracy: 0.889, loss: 5.965815544128417\n",
            "epoch: 6, validation accuracy: 0.8937, loss: 5.653503303527832\n",
            "epoch: 7, validation accuracy: 0.8959, loss: 5.621636781692505\n",
            "epoch: 8, validation accuracy: 0.9058, loss: 4.810211319923401\n",
            "epoch: 9, validation accuracy: 0.9142, loss: 5.174941515922548\n",
            "epoch: 10, validation accuracy: 0.9125, loss: 3.8908423376083365\n",
            "epoch: 11, validation accuracy: 0.8658, loss: 4.601394052505493\n",
            "epoch: 12, validation accuracy: 0.877, loss: 4.431019220352173\n",
            "epoch: 13, validation accuracy: 0.9043, loss: 4.767708916664124\n",
            "epoch: 14, validation accuracy: 0.6411, loss: 33.19531376838684\n",
            "epoch: 15, validation accuracy: 0.9119, loss: 15.19261353492737\n",
            "epoch: 16, validation accuracy: 0.9235, loss: 5.954882097244263\n",
            "epoch: 17, validation accuracy: 0.919, loss: 4.510854606628417\n",
            "epoch: 18, validation accuracy: 0.9192, loss: 3.7836524009704577\n",
            "epoch: 19, validation accuracy: 0.9186, loss: 3.3431189346313483\n",
            "epoch: 20, validation accuracy: 0.9119, loss: 3.137766578197478\n",
            "epoch: 21, validation accuracy: 0.9147, loss: 3.1207399797439574\n",
            "epoch: 22, validation accuracy: 0.9143, loss: 2.829768464565278\n",
            "epoch: 23, validation accuracy: 0.9114, loss: 2.5556430721282957\n",
            "epoch: 24, validation accuracy: 0.9151, loss: 2.557132253646852\n",
            "epoch: 25, validation accuracy: 0.9203, loss: 2.46116994857788\n",
            "epoch: 26, validation accuracy: 0.9206, loss: 2.554994585514069\n",
            "epoch: 27, validation accuracy: 0.885, loss: 2.374559874534607\n",
            "epoch: 28, validation accuracy: 0.9196, loss: 2.4558713459968566\n",
            "epoch: 29, validation accuracy: 0.9187, loss: 2.0474378633499146\n",
            "[Test Accuracy] : 0.9185\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}