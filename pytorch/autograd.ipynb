{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch - Autograd\n",
    "autograd는 신경망 학습을 지원하는 pytorch의 자동 미분 엔진이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda decive:  1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print(\"Current cuda decive: \", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # 파라미터를 업데이트 할 때 계산되는 데이터의 개수\n",
    "input_size = 1000\n",
    "hidden_size = 100\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, input_size, device=device, dtype = torch.float, requires_grad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- randn : 평균이 0, 표준편차가 1인 정규분포에서 샘플링한 값으로, 데이터를 만든다는 것을 의미. batch_size, input_size에 따라 데이터의 모양 달라짐.  \n",
    "- x는 input으로 이용되기 때문에 gradient 계산할 필요 없다. 따라서 requires_grade = False  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(batch_size, output_size, device=device, dtype=torch.float, requires_grad=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- output 설정. batch_size 만큼 결과값 필요.\n",
    "- output과의 오차를 계산하기 위해 output의 크기를 10으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(input_size, hidden_size, device = device, dtype = torch.float,requires_grad = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 업데이트 할 파라미터 값 설정\n",
    "- input data 크기 1000이며 이것과 행렬곱을 하므로 다음 행의 값이 1000이어야 함.\n",
    "- 행렬곱을 이용해 100 크기의 데이터를 생성하기 위해 (1000, 100)크기의 데이터를 생성.\n",
    "- x, y와 다르게 gradient를 계산해야 하므로 requires_grad = True로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = torch.randn(hidden_size,output_size, device = device,dtype = torch.float,requires_grad = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- w1과 x를 행렬 곱한 결과에 계산할 수 있는 데이터여야 한다.\n",
    "- w1과 x의 행렬 곲을 한 결과는 (1,100)이며 (100, 10)행렬을 통해 output을 계산할 수 있도록 한다.\n",
    "- backpropagation을 통해 업데이트해야하는 대상이므로 requires_grad = True로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  100 \t Loss:  309.1791687011719\n",
      "Iteration:  200 \t Loss:  1.4110450744628906\n",
      "Iteration:  300 \t Loss:  0.011791206896305084\n",
      "Iteration:  400 \t Loss:  0.0002832590544130653\n",
      "Iteration:  500 \t Loss:  4.207486199447885e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6     \n",
    "for i in range(1, 501):\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()  # 제곱차의 합\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration: \", i, \"\\t\", \"Loss: \", loss.item()) \n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():                                      \n",
    "        w1 -= learning_rate * w1.grad                          \n",
    "        w2 -= learning_rate * w2.grad                          \n",
    "\n",
    "        w1.grad.zero_()                                      \n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clamp : 값의 상한선/하한선 설정 (min=0이므로 0보다 작은 값이 발생하면 그 값을 0으로 바꿔줌 = ReLU와 같은 역할.)\n",
    "- pow : 지수를 의미. (2)이므로 제곱.\n",
    "- backward : 계산된 loss 값에 대해 사용하면 각 파라미터 값에 대해 gradient를 계산하고 이를 통해 back propagation을 진행한다는 것을 의미.\n",
    "- with절 : 코드가 실행되는 시점에서 gradient값을 고정한다는 의미\n",
    "- grad.zero() : gradient값을 0으로 설정. 다음 backpropagation을 진행할 때 gradient 값을 loss.backward()를 통해 새로 계산하기 때문"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
